<!DOCTYPE html>
<html lang="en">
    <head>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>AI LLMs</title>
    </head>
    <style>
        body{
            padding: 0px;
            justify-content: center;
            align-items: center;
            transition: opacity 1s ease-in;
            color: rgb(10, 10, 10);
            margin: 0;
            background-image: url('llm.jpg');
            background-size: cover;
            background-position: center;
            transition: opacity 0.5s ease-in-out;
        }
        .fade-in{
            opacity: 0;
        }
        .container{
            width: 60%;
            margin: 0 auto;
            text-align: justify;
            padding: 20x;
            border: 0.2cm solid black;
            background-color: rgb(106, 118, 167);
        }
    </style>
    <body>
        <div class="container">
            <h1 style="text-align: center; color: rgb(197, 191, 40);">AI LLMs</h1>
            <p style="color: black;">
                Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks.
                <br><br>
                <div style="text-align: center;color: black;">
                    <img src="https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F9770082%2Fb719945a46e76b182610df73d2a30fb2%2Froadmap-1.jpg?generation=1701538614516917&alt=media" width="300"  height="300" style="text-align: center;"><br>
                    <p style="text-align: center;">(Figure 1: - Evolution of LLMs)</p>
                    <a href="https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.kaggle.com%2Fdiscussions%2Fgeneral%2F458973&psig=AOvVaw0OBZ6_1oyMntoz81q2ZW-b&ust=1722589932319000&source=images&cd=vfe&opi=89978449&ved=0CBQQjhxqFwoTCKD7wtq504cDFQAAAAAdAAAAABAEhttps://explodingtopics.com/blog/ai-statistics">Click here for source</a><br><br>
                </div>
                1. Early Days: chatbots and rule-based systems (1960s)-<br> 
                In the 1960s, the first chatbots like ELIZA were created, utilizing simple rule-based systems to simulate human conversation. These early AI models relied heavily on pre-defined scripts and lacked the ability to understand context or learn from interactions.
                <br><br>
                2. Rise of Recurrent Neural Networks (1980s)-<br>
                Recurrent Neural Networks (RNNs) emerged in the 1980s, introducing a new way for AI systems to handle sequential data. RNNs could process input sequences of variable length, making them suitable for tasks like language modeling and time series prediction.
                <br><br>
                3. Rise of Long Short Term Memory (1990s)-<br>
                In the 1990s, Long Short-Term Memory (LSTM) networks were developed to address the limitations of traditional RNNs, particularly their difficulty in learning long-term dependencies. LSTMs improved the handling of sequential data by incorporating mechanisms to retain information over extended periods.
                <br><br>
                4. Gated Recurrent Network (2010s)-<br>
                The 2010s saw the development of Gated Recurrent Units (GRUs), a simpler and more efficient alternative to LSTMs. GRUs offered comparable performance with fewer parameters, making them a popular choice for various natural language processing tasks.
                <br><br>
                5. Rise of Attention Mechanism (2014)-<br>
                The introduction of the attention mechanism in 2014 revolutionised the way neural networks processed sequential data. Attention mechanisms allowed models to focus on specific parts of the input sequence, enhancing their ability to capture relevant information and improving performance on tasks like machine translation.
                <br><br>
                6. The invention of Transformers Architecture (2017)-<br>
                In 2017, the transformer architecture was introduced, marking a significant breakthrough in AI. Unlike RNNs and LSTMs, transformers relied entirely on attention mechanisms, enabling parallel processing of data and leading to substantial improvements in efficiency and accuracy for tasks such as language translation and text generation.
                <br><br>
                7. Emergence of Large Language Models (2018-onwards)-<br>
                From 2018 onwards, large language models (LLMs) like GPT-3 and BERT emerged, showcasing unprecedented capabilities in understanding and generating human language. These models, built on transformer architecture, have been trained on vast amounts of data and have demonstrated remarkable proficiency across a wide range of natural language processing tasks. 
                <br><br>
                <div style="text-align: center;">
                    <img src="https://media.licdn.com/dms/image/D4D12AQGjVz-3CjYtrQ/article-cover_image-shrink_600_2000/0/1696058828180?e=2147483647&v=beta&t=l9uFu9MvGINYGliBx7ztju8ZahzfwcGB6bZh_VuLHaE" width="300"  height="300" style="text-align: center;"><br>
                    <p style="text-align: center;">(Figure 2: - Evolution of LLMs)</p>
                    <a href="https://www.nextbigfuture.com/2023/04/timeline-of-open-and-proprietary-large-language-models.html">Click here for source</a><br><br>
                    <br>
                </div>
                <br>
                <b>Usage of LLMs</b><br>
                1. Search-Search engines like Google and Bing already use LLMs to offer better user results. Search Engines achieve this by understanding the user’s search intent and using that information to provide the most relevant & direct results.
                <br><br>
                2. Generate Content - <br>
                i. Content creation: LLMs can create new content for blogs, social media, and other digital platforms. This could mean using existing content as a starting point and making new text related to the original content, or it could mean making new content based on a set of keywords or other input.
                <br>ii. Dialogue generation: LLMs can make chatbots, virtual assistants, and other conversational agents talk to each other. This could mean coming up with answers to user questions based on a knowledge base or database of solutions or coming up with a new dialogue that fits the needs or preferences of a specific user.
                <br>iii. Storytelling: LLMs can be used to develop new stories or stories with a particular theme or prompt. This could mean making short stories or longer works of fiction, or it could mean making stories that are geared toward one specific audience or goal.
                <br>iv. Feed for TTS: LLMs can make text feed in different languages that sound natural. This ability can make the TTS system more robust and automated.
                <br>v. Content augmentation: LLMs can add to existing content by making more context — or detail-rich text. This could mean adding to articles, reports, or other documents that are already out there or making summaries or abstracts that provide a high-level overview of the content.
                <br><br>
                3.Extraction from data sets: LLMs have the capability to extract information from large amounts of unstructured data, such as posts on social media or customer reviews.
                <br><br>
                4.Expand the content:
                LLMs can expand on existing content by generating additional paragraphs, sentences, or ideas. For expansion, LLM can use techniques such as semantic similarity and text generation to produce new content related to the original text.
                <br><br>
                5.Market Research and Competitor Analysis: When making a content strategy or launching a new product, it’s essential to research the market. The information gathered often determines what is written about and how it is told. Language models like LLMs can help get and look at the correct data for market research and competitor analysis.
                <br><br>
                <div style="text-align: center;">
                    <p>We have also conducted a survey to understand people's view on LLMs. Totally we have received 51 responses. Here are the stats: - <br></p>
                    <img src="image.png" width="300" height="200"><br>
                    <img src="image2.png" width="300" height="200"><br>
                    <img src="image3.png" width="300" height="200"><br>
                    <img src="image4.png" width="300" height="200"><br>
                    <img src="image5.png" width="300" height="200"><br>
                    <a href="https://docs.google.com/forms/d/e/1FAIpQLSdhL6BnvEoZnwnv8eZW4YrQwDaJRPXhN2LeR6C_XUaouYQE8A/viewform?usp=sf_link">Forms Link</a><br><br>
                    <button onclick="navigateTo('index.html')" class="btn btn-primary" style="background-color: rgb(11, 179, 191);">Back to Home</button>
                    <button onclick="navigateTo('upsurge.html')" class="btn btn-primary" style="background-color: rgb(188, 188, 65);">Upsurge</button>
                    <button onclick="navigateTo('creation.html')" class="btn btn-primary" style="background-color: rgb(237, 84, 84);">Creation</button>
                    <button onclick="navigateTo('usage.html')" class="btn btn-primary" style="background-color: rgb(11, 191, 68);">Usage</button>
                </div>
            </p>
        </div>
        <script>
            function navigateTo(href) {
              document.body.classList.add('fade-in');
              setTimeout(function() {
                window.location = href;
                }, 500);
            }
          </script>
    </body>
</html>